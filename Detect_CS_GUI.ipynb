{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detect_CS_GUI",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeFalko/detect_CS/blob/master/Detect_CS_GUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs9VmuZpy9ca",
        "outputId": "a02b90fc-fc06-4d1f-cf25-8bb202376c74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjsWSspuo64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c178e407-b82c-4f82-9dbc-35e5c0a2f6b5"
      },
      "source": [
        "import os\n",
        "\n",
        "#download U'n'Eye\n",
        "!git clone https://github.com/berenslab/uneye.git\n",
        "\n",
        "# remove folders and files that are not needed\n",
        "os.chdir('uneye')\n",
        "import shutil\n",
        "files = ['README.md','UnEye.ipynb','logo.jpeg','requirements_lin.txt','requirements_mac.txt','requirements_wind.txt','setup.py']\n",
        "for i,f in enumerate(files):\n",
        "    try:\n",
        "        os.remove(f)\n",
        "    except:\n",
        "        print(\"File already deleted: \", f)\n",
        "          \n",
        "folders = ['.git','analysis scripts','data']\n",
        "for i,f in enumerate(folders) :\n",
        "    try:\n",
        "        shutil.rmtree(f)\n",
        "    except:\n",
        "        print(\"Folder already deleted: \", f)\n",
        "os.makedirs('training',exist_ok = True)\n",
        "os.makedirs('work',exist_ok = True)\n",
        "os.makedirs('work/LabelYourData',exist_ok = True)\n",
        "os.makedirs('work/Output',exist_ok = True)\n",
        "os.makedirs('work/TrainYourNetwork',exist_ok = True)\n",
        "\n",
        "# add a patch to use U'n'Eye as CS detector\n",
        "!git clone https://github.com/LeFalko/detect_CS.git\n",
        "\n",
        "# to do math operations\n",
        "import numpy as np\n",
        "import scipy.io as io\n",
        "\n",
        "# to read .mat files\n",
        "!pip install mat4py\n",
        "!pip install mat73\n",
        "import mat4py\n",
        "# to do read .pkl files\n",
        "import pandas as pd\n",
        "\n",
        "# to do clustering\n",
        "!pip install hdbscan\n",
        "\n",
        "import uneye\n",
        "!pip install umap-learn\n",
        "from detect_CS import *\n",
        "#to download files from the internet\n",
        "import urllib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'uneye'...\n",
            "remote: Enumerating objects: 990, done.\u001b[K\n",
            "remote: Total 990 (delta 0), reused 0 (delta 0), pack-reused 990\u001b[K\n",
            "Receiving objects: 100% (990/990), 169.38 MiB | 23.58 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "Checking out files: 100% (489/489), done.\n",
            "Cloning into 'detect_CS'...\n",
            "remote: Enumerating objects: 547, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 547 (delta 122), reused 94 (delta 41), pack-reused 351\u001b[K\n",
            "Receiving objects: 100% (547/547), 2.13 MiB | 3.36 MiB/s, done.\n",
            "Resolving deltas: 100% (329/329), done.\n",
            "Collecting mat4py\n",
            "  Downloading mat4py-0.5.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: mat4py\n",
            "Successfully installed mat4py-0.5.0\n",
            "Collecting mat73\n",
            "  Downloading mat73-0.58-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from mat73) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mat73) (1.21.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->mat73) (1.5.2)\n",
            "Installing collected packages: mat73\n",
            "Successfully installed mat73-0.58\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.1.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.28)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330729 sha256=efae7c2d730e6c3b4b26bd384548c0a06bc8d97558538bcb786f230aae0fc126\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.28\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=baaa9ddd75e1590556290780335db2a55f288c7dafa849adaeb439e682b79e07\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=20597742552dc7a106b3d953097fd8520a36aa6d3c2939d5f0ddeaa5d664b9e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PaI-8cn9dCw"
      },
      "source": [
        "file_link = 'https://ndownloader.figshare.com/files/21765003?private_link=a1f3eb9510b0a628ed28'\n",
        "\n",
        "urllib.request.urlretrieve(file_link,'PC_EMI_S4_Right_10-08-2017.mat') # Download the file from Figshare server\n",
        "example_file = mat4py.loadmat('PC_EMI_S4_Right_10-08-2017.mat') # Load the file in memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn2t-8Lv_PLP",
        "outputId": "2207abd2-f3f2-45de-8bbf-47bbc74982af"
      },
      "source": [
        "print(example_file.keys())\n",
        "sampling_rate = 25000; # Hz\n",
        "\n",
        "RAW = np.array(example_file['RAW'])\n",
        "HIGH = np.array(example_file['HIGH'])\n",
        "Labels = np.array(example_file['Labels'])\n",
        "Interval_inspected = np.array(example_file['Interval_inspected'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['RAW', 'HIGH', 'Labels', 'Interval_inspected'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJj37XQ7_SSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b474588f-7403-4c39-82b0-c520426c778e"
      },
      "source": [
        "sampling_rate = 25000; # Hz\n",
        "\n",
        "LFP,High,Labels,Interval_inspected = load_data(filename = 'PC_EMI_S4_Right_10-08-2017.mat',# path of the file\n",
        "                          # it can be a .mat file, a .pkl file or a .csv file\n",
        "                          # set to [] if you use .csv files\n",
        "                            field_LFP = ['RAW'],# in a .pkl file where the LFP data is in the key ['lfp'] then set: field_LFP = 'lfp'\n",
        "                            # in a .mat file where the LFP data is in the variable lfp then set: field_LFP = 'lfp'\n",
        "                            # in a .mat file where the LFP data is in the structure Data.lfp then set: field_LFP = ['Data','lfp']\n",
        "                            # with .csv files this is the path to the file containing LFP\n",
        "                            # load data will also automatically filter the wide-band signal if it is provided instead of the LFP hence we give the ['RAW'] field\n",
        "                            field_high_pass = ['HIGH'], # same as above but the data has to be already high-passed\n",
        "                            field_label = ['Labels'], # same as above\n",
        "                            field_intervs = ['Interval_inspected'], # same as above\n",
        "                            sampling_freq = sampling_rate # sampling frequency in Hz\n",
        "                            )\n",
        "compLFP,compHIGH,compLabels = concatenate_segments(LFP,High,Interval_inspected,Labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.18677687 -0.17702209 -0.16890409 ...  0.04995807  0.03908252\n",
            "  0.0301529 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plKVE-vH_SLt"
      },
      "source": [
        "# Download the training set of our study\n",
        "# It is already normalized and concatenated\n",
        "file_link ='https://ndownloader.figshare.com/files/21809856?private_link=31e490395584c1eea69b'\n",
        "urllib.request.urlretrieve(file_link,'training_set.mat') # The file training_set.mat contains all normalized and concatenated segments from our dataset\n",
        "\n",
        "DATA = io.loadmat('training_set.mat')\n",
        "\n",
        "ID = DATA['ID']\n",
        "bigTraining_LFP = DATA['LFP'].squeeze()\n",
        "bigTraining_HIGH = DATA['HIGH'].squeeze()\n",
        "bigTraining_LABEL = DATA['LABEL'].squeeze()\n",
        "\n",
        "def squizz(data):\n",
        "    data = [x.squeeze() for x in data]\n",
        "    return(data)\n",
        "bigTraining_LFP = squizz(bigTraining_LFP)\n",
        "bigTraining_HIGH = squizz(bigTraining_HIGH)\n",
        "bigTraining_LABEL = squizz(bigTraining_LABEL)\n",
        "DATA = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFrxuPaZ_SBQ",
        "outputId": "20759dd6-49fd-454e-868a-0d67766dd406"
      },
      "source": [
        "length = int(sampling_rate*1) # we will use only 1 s of data from each cell\n",
        "trainingLFP = []\n",
        "trainingHIGH = []\n",
        "trainingLabel = []\n",
        "for ii,txt in enumerate(ID):\n",
        "    if not(txt == 'PC_EMI_S4_Right_10-08-2017'):\n",
        "            trainingLFP.append(bigTraining_LFP[ii][:length])\n",
        "            trainingHIGH.append(bigTraining_HIGH[ii][:length])\n",
        "            trainingLabel.append(bigTraining_LABEL[ii][:length])\n",
        "    else:\n",
        "        print('PC_EMI_S4_Right_10-08-2017 is skipped')\n",
        "trainingLFP = np.concatenate(trainingLFP)  \n",
        "trainingHIGH = np.concatenate(trainingHIGH)  \n",
        "trainingLabel = np.concatenate(trainingLabel)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC_EMI_S4_Right_10-08-2017 is skipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSg-ydzgGI4e",
        "outputId": "12b51c28-488c-4038-d0ef-2ac995823ce1"
      },
      "source": [
        "# size of the convolutional and maxpooling operations\n",
        "ks = 9 # convolution kernel size. NEED TO BE ODD.\n",
        "mp = 7 # maxpooling size. NEED TO BE ODD.\n",
        "\n",
        "# the number of bins (nb) taken into account by the network is given by\n",
        "# nb = mp**2+mp**2*ks+(mp*ks)-mp+2*ks-2 \n",
        "\n",
        "weights_name = 'Test' # the weights will be saved in the folder 'training'\n",
        "sampfreq = 25000 # Sampling frequency of your signal in Hz\n",
        "val_samples = 10 # If you have a small training set you should consider reducing this number.\n",
        "                 # Otherwise the training will show an error\n",
        "#########################\n",
        "model = uneye.DNN(max_iter = 3000, ks=ks,mp=mp, weights_name=weights_name,\n",
        "                  sampfreq = sampfreq,augmentation = False,val_samples = val_samples, doDiff = False)\n",
        "\n",
        "model.train(trainingLFP,trainingHIGH,trainingLabel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2\n",
            "Using GPU: True\n",
            "Training. Please wait.\n",
            "Early stopping at epoch 99 before overfitting occurred.\n",
            "Model parameters saved to ./training/Test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<uneye.classifier.DNN at 0x7fd6dff60fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obNJC_SgFcF4"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I1FqIlzLb1Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jjOblkBCgdL"
      },
      "source": [
        "# size of the convolutional and maxpooling operations\n",
        "ks = 9 # convolution kernel size. NEED TO BE ODD.\n",
        "mp = 7 # maxpooling size. NEED TO BE ODD.\n",
        "\n",
        "# the number of bins (nb) taken into account by the network is given by\n",
        "# nb = mp**2+mp**2*ks+(mp*ks)-mp+2*ks-2 \n",
        "\n",
        "weights_name = 'Test' # the weights will be saved in the folder 'training'\n",
        "sampfreq = 25000 # Sampling frequency of your signal in Hz\n",
        "val_samples = 10 # If you have a small training set you should consider reducing this number.\n",
        "                 # Otherwise the training will show an error\n",
        "field_LFP = ['RAW']\n",
        "field_high_pass = ['HIGH']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At_o-WPiL0x7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "fec6cc5d-0b68-4305-b220-475ade3b7b37"
      },
      "source": [
        "testing_folder = '/gdrive/My Drive/uneye/work/LabelYourData/'\n",
        "testing_files = os.listdir(testing_folder)\n",
        "output_folder = '/gdrive/My Drive/uneye/work/Output/'\n",
        "for tf in testing_files:\n",
        "    LFP,High,_,_ = load_data(filename = testing_folder+tf,field_LFP = field_LFP,field_high_pass = field_high_pass)\n",
        "    \n",
        "    output = detect_CS(weights_name,\n",
        "                   LFP,\n",
        "                   High,\n",
        "                   # The arguments above need to be given is this order\n",
        "                \n",
        "                   # The arguments bellow are optional\n",
        "\n",
        "                   output_name = 'results.mat', # Here you write the path and name of the\n",
        "                                       # file to save a .mat file of the results\n",
        "                                       # default value: None , \n",
        "                   \n",
        "                   sampling_frequency = sampfreq, # In Hz. Default value: 25000\n",
        "                   ks=ks, # convolution kernel size. NEED TO BE ODD. Default value: 9\n",
        "                   mp=mp, # maxpooling size. NEED TO BE ODD. Default value: 7\n",
        "                   realign = True, # Realign waveforms in the range given by the variable 'alignment_w' before clustering\n",
        "                                   # default value: True\n",
        "                   alignment_w = (-.5,2), # Time (ms) before and after the putative CS onset used to realign onset\n",
        "                                         # default value: (-.5,2)\n",
        "                   cluster = True, # Perform dimensionality reduction and clustering in the window given by the parameter 'cluster_w'\n",
        "                                  # default value: True\n",
        "                   cluster_w = (0,2), # Time (ms) before and after the putative CS onset used to perform dimensionality reduction\n",
        "                                      # default value: (0,2)\n",
        "                   plot_w= (-4,8), # Time (ms) before and after the putative CS onset used to display waveforms\n",
        "                                    # default value: (-4,8)\n",
        "                   exlude_w = 3, # Minimum interval (in ms) of high predictive probability to consider a cluster of putative CSs as a real CSs\n",
        "                                # default value: 3\n",
        "                   plot = True, # Display waveforms (as in figure 2 of our paper)\n",
        "                                # default value: False\n",
        "                   plot_only_good = False, # If set to True, only the putative CSs kept by the algorithm are displayed\n",
        "                                           # If set to False, the events that are too short are also displayed\n",
        "                                           # default value: True\n",
        "                   )\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-52a79af6995a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                    \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Display waveforms (as in figure 2 of our paper)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                 \u001b[0;31m# default value: False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                    \u001b[0mplot_only_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# If set to True, only the putative CSs kept by the algorithm are displayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                                            \u001b[0;31m# If set to False, the events that are too short are also displayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                            \u001b[0;31m# default value: True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/uneye/detect_CS/CS.py\u001b[0m in \u001b[0;36mdetect_CS\u001b[0;34m(weights_name, LFP, High_passed, output_name, sampling_frequency, ks, mp, exlude_w, realign, alignment_w, cluster, cluster_w, plot, plot_w, plot_only_good)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if GPU, process all segments at ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mPred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLFP_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mHigh_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLFP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mtrial_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/uneye/uneye/classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mout_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./training'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mw_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'c7.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './training/Test'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gDAISlHB60v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}